"""
Generates predictions using a trained GraphSAGE model on a target list.

This script executes Part 3 of the pipeline: Prediction.
It performs the following steps:
1.  Loads the pre-processed features and account mappings (from Part 1).
2.  Loads the raw transaction data to reconstruct the full graph structure.
3.  Loads the list of accounts requiring predictions.
4.  Re-defines the GraphSAGE model architecture (must match Part 2).
5.  Loads the saved weights of the best trained model (from Part 2).
6.  Performs a forward pass on the entire graph to get probabilities for all
    nodes.
7.  Extracts the "suspicious" probabilities for the target accounts.
8.  Applies an optimal threshold to classify accounts as 0 (normal) or
    1 (suspicious).
9.  Saves the final results to 'submission.csv'.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data
from torch_geometric.utils import to_undirected
import pandas as pd
import numpy as np

### --- 0. Environment Setup and Model Definition ---
print("--- Part 3: Prediction ---")
print("Step 0: Setup Environment and Model Definition...")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Model definition (must be identical to Part 2)
class GraphSAGE(nn.Module):
    """
    A 3-layer GraphSAGE model with Batch Normalization and Dropout.

    This model implements a stack of SAGEConv layers for node classification.
    The architecture is:
    SAGEConv -> BatchNorm -> ReLU -> Dropout
    SAGEConv -> BatchNorm -> ReLU -> Dropout
    SAGEConv (linear output)

    Args:
        in_channels (int): Dimensionality of the input node features.
        hidden_channels (int): Dimensionality of the hidden embeddings.
        out_channels (int): Dimensionality of the output (number of classes).
        dropout (float): Dropout probability.

    Attributes:
        conv1 (SAGEConv): The first graph convolutional layer.
        bn1 (nn.BatchNorm1d): Batch normalization for the first hidden layer.
        conv2 (SAGEConv): The second graph convolutional layer.
        bn2 (nn.BatchNorm1d): Batch normalization for the second hidden layer.
        conv3 (SAGEConv): The output graph convolutional layer.
        dropout (nn.Dropout): Dropout layer.
    """
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):
        """Initializes the GraphSAGE model layers."""
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.bn1 = nn.BatchNorm1d(hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.bn2 = nn.BatchNorm1d(hidden_channels)
        self.conv3 = SAGEConv(hidden_channels, out_channels)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        """
        Defines the forward pass of the GraphSAGE model.

        Args:
            x (torch.Tensor): The input node feature tensor.
            edge_index (torch.Tensor): The graph's edge index.

        Returns:
            torch.Tensor: The raw output logits for each node.
        """
        x = F.relu(self.bn1(self.conv1(x, edge_index)))
        x = self.dropout(x)
        x = F.relu(self.bn2(self.conv2(x, edge_index)))
        x = self.dropout(x)
        x = self.conv3(x, edge_index)
        return x

### --- 1. Loading Necessary Data ---
print("\nStep 1: Loading necessary data...")
try:
    # Load the correct feature file generated by Part 1 (with interaction features)
    feature_file = 'processed_features_with_interactions.pt'
    saved_data = torch.load(feature_file, map_location=device, weights_only=False)
    
    feature_tensor = saved_data['feature_tensor']
    acct_to_idx = saved_data['acct_to_idx']
    print(f"Pre-processed features '{feature_file}' and mappings loaded.")
except FileNotFoundError:
    print(f"Error: '{feature_file}' not found. Please run Part 1 first.")
    exit()

try:
    # ðŸŽ¯ Ensure these paths are correct
    predict_df = pd.read_csv("data/acct_predict.csv", dtype={'acct': str})
    df_trans = pd.read_csv("data/acct_transaction.csv", dtype={'from_acct': str, 'to_acct': str})
    print("Prediction account list and raw transactions loaded.")
except FileNotFoundError as e:
    print(f"Error loading data files: {e}")
    exit()

### --- 2. Reconstructing the Full Graph ---
print("\nStep 2: Reconstructing the full graph...")
# Ensure only accounts present in the feature mapping are used to build edges
valid_trans = df_trans[df_trans['from_acct'].isin(acct_to_idx) & df_trans['to_acct'].isin(acct_to_idx)]
source_nodes = valid_trans['from_acct'].map(acct_to_idx).values
target_nodes = valid_trans['to_acct'].map(acct_to_idx).values
edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)
edge_index = to_undirected(edge_index) # Maintain consistency with training

# Create the Data object using the feature_tensor loaded in Step 1
full_graph_data = Data(x=feature_tensor, edge_index=edge_index).to(device)
print(f"Full graph data object reconstructed ({full_graph_data.num_nodes} nodes).")

### --- 3. Loading Trained Model and Making Predictions ---
print("\nStep 3: Loading trained model and making predictions...")
# Hyperparameters must match those used in Part 2
HIDDEN_CHANNELS = 768 
model = GraphSAGE(in_channels=full_graph_data.num_node_features, 
                  hidden_channels=HIDDEN_CHANNELS, 
                  out_channels=2).to(device)

try:
    # Load the trained model weights from Part 2
    model_file = 'best_model.pt'
    model.load_state_dict(torch.load(model_file, map_location=device))
    print(f"Trained model weights loaded from '{model_file}'.")
except FileNotFoundError:
    print(f"Error: '{model_file}' not found. Please run Part 2 to train the model first.")
    exit()

model.eval()
with torch.no_grad():
    all_logits = model(full_graph_data.x, full_graph_data.edge_index)
    all_probs = F.softmax(all_logits, dim=1)
print("Predictions generated for all nodes in the graph.")

### --- 4. Extracting Results and Generating Submission File ---
print("\nStep 4: Extracting results and generating submission file...")

OPTIMAL_THRESHOLD = 0.4625
print(f"Using threshold: {OPTIMAL_THRESHOLD}")

# Get the "class 1" (suspicious) probabilities for all nodes
all_pos_probs = all_probs[:, 1].cpu().numpy()

# Get the indices for the accounts in the prediction list
predict_indices = [acct_to_idx.get(acct, -1) for acct in predict_df['acct']]

target_pos_probs = []
for idx in predict_indices:
    if idx != -1:
        # If the account is in the graph, get its suspicious probability
        target_pos_probs.append(all_pos_probs[idx])
    else:
        # If the account is not in the graph (e.g., a new account in 'acct_predict'),
        # default its suspicious probability to 0.0 (label 0)
        target_pos_probs.append(0.0)

# Use the OPTIMAL_THRESHOLD to determine the final labels
target_pos_probs_np = np.array(target_pos_probs)
pred_labels = (target_pos_probs_np > OPTIMAL_THRESHOLD).astype(int)

# Generate the submission file
submission_df = predict_df[['acct']].copy()
submission_df['label'] = pred_labels
submission_df.to_csv("submission.csv", index=False)

print("\nsubmission.csv created successfully!")
print("Predicted label distribution (based on threshold):")
print(pd.Series(pred_labels).value_counts())
print("\n--- Part 3 Completed ---")