{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import gc\n",
    "from scipy import sparse\n",
    "\n",
    "\"\"\"\n",
    "Financial Transaction Feature Engineering Pipeline\n",
    "\n",
    "This script performs high-performance, vectorized feature engineering on financial \n",
    "transaction data. It generates:\n",
    "1. Basic aggregation features (sum, mean, std, counts).\n",
    "2. Advanced temporal features (linear trends, volatility, density).\n",
    "3. Network/Graph features (PageRank, degrees) using sparse matrices.\n",
    "4. Interaction features (ratios, net flows).\n",
    "5. Registered/Whitelist features (Static connection counts and transactional verification).\n",
    "\n",
    "The output is a normalized PyTorch tensor suitable for Graph Neural Networks \n",
    "or Tabular models.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def fast_pagerank(A, d=0.85, tol=1e-4, max_iter=20):\n",
    "    \"\"\"\n",
    "    Calculates PageRank using a sparse adjacency matrix and power iteration.\n",
    "    \n",
    "    Args:\n",
    "        A (scipy.sparse.coo_matrix): Adjacency matrix of the graph.\n",
    "        d (float): Damping factor.\n",
    "        tol (float): Convergence tolerance.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: PageRank scores for all nodes.\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Normalize matrix: Row stochastic\n",
    "    out_degree = np.array(A.sum(axis=1)).flatten()\n",
    "    out_degree[out_degree == 0] = 1.0 \n",
    "    D_inv = sparse.diags(1.0 / out_degree)\n",
    "    M = D_inv @ A \n",
    "    \n",
    "    # Initialization\n",
    "    r = np.ones(n) / n\n",
    "    teleport = np.ones(n) / n\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        r_new = d * (M.T @ r) + (1 - d) * teleport\n",
    "        if np.linalg.norm(r_new - r, 1) < tol:\n",
    "            break\n",
    "        r = r_new\n",
    "    return r\n",
    "\n",
    "### --- 0. GPU Detection ---\n",
    "print(\"--- Part 1: Feature Engineering ---\")\n",
    "print(\"Step 0: Checking for GPU...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU '{torch.cuda.get_device_name(0)}' is available.\")\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\")\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "print(\"\\nStep 1: Loading data...\")\n",
    "try:\n",
    "    # Attempt to use pyarrow engine for faster IO, fallback to standard C engine on failure\n",
    "    try:\n",
    "        df_trans = pd.read_csv(\"data/acct_transaction.csv\", dtype={'from_acct': str, 'to_acct': str}, engine='pyarrow')\n",
    "        df_alert = pd.read_csv(\"data/acct_alert.csv\", dtype={'acct': str}, engine='pyarrow')\n",
    "        # [NEW] Load Register Data\n",
    "        df_reg = pd.read_csv(\"data/acct_register.csv\", dtype={'from_acct': str, 'to_acct': str}, engine='pyarrow')\n",
    "    except:\n",
    "        print(\"Pyarrow engine not found, using default...\")\n",
    "        df_trans = pd.read_csv(\"data/acct_transaction.csv\", dtype={'from_acct': str, 'to_acct': str})\n",
    "        df_alert = pd.read_csv(\"data/acct_alert.csv\", dtype={'acct': str})\n",
    "        # [NEW] Load Register Data\n",
    "        df_reg = pd.read_csv(\"data/acct_register.csv\", dtype={'from_acct': str, 'to_acct': str})\n",
    "        \n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "print(\"\\nStep 2: Performing vectorized feature engineering...\")\n",
    "\n",
    "# Pre-processing temporal columns\n",
    "df_trans['txn_time'] = pd.to_datetime(df_trans['txn_time'], format='%H:%M:%S', errors='coerce')\n",
    "df_trans['txn_hour'] = df_trans['txn_time'].dt.hour\n",
    "df_trans['is_late_night'] = ((df_trans['txn_hour'] >= 0) & (df_trans['txn_hour'] < 6)).astype(int)\n",
    "df_trans['txn_date'] = pd.to_datetime(df_trans['txn_date'], errors='coerce')\n",
    "\n",
    "# Convert dates to numeric index \n",
    "# Note: Aligning with register data where Day 1 is the start.\n",
    "min_date = df_trans['txn_date'].min()\n",
    "# We add +1 so the first day is 1, matching the register file logic\n",
    "df_trans['day_idx'] = (df_trans['txn_date'] - min_date).dt.days + 1 \n",
    "\n",
    "# One-hot encoding and logic checks\n",
    "df_trans = pd.get_dummies(df_trans, columns=['channel_type'], prefix='channel', dtype=int)\n",
    "df_trans['is_round_number_txn'] = (df_trans['txn_amt'] % 1000 == 0).astype(int)\n",
    "\n",
    "# ==========================================\n",
    "# [NEW] Processing Register (Whitelist) Logic\n",
    "# ==========================================\n",
    "print(\"Processing Register (Whitelist) Features...\")\n",
    "\n",
    "# 1. Clean Register Data (Clamp dates as per instructions)\n",
    "max_txn_day = df_trans['day_idx'].max()\n",
    "df_reg['start_date'] = pd.to_numeric(df_reg['start_date'], errors='coerce').fillna(-1).astype(int)\n",
    "df_reg['end_date'] = pd.to_numeric(df_reg['end_date'], errors='coerce').fillna(999).astype(int)\n",
    "\n",
    "# Logic: start_date < 1 -> -1, end_date > max -> 999\n",
    "df_reg.loc[df_reg['start_date'] < 1, 'start_date'] = -1\n",
    "df_reg.loc[df_reg['end_date'] > max_txn_day, 'end_date'] = 999\n",
    "\n",
    "# 2. Static Register Features (Trust Graph Structure)\n",
    "# Count how many beneficiaries this account has registered (Trusted Out-degree)\n",
    "reg_static_out = df_reg.groupby('from_acct')['to_acct'].nunique().to_frame('num_registered_beneficiaries')\n",
    "# Count how many people have registered this account (Trusted In-degree)\n",
    "reg_static_in = df_reg.groupby('to_acct')['from_acct'].nunique().to_frame('num_registered_sources')\n",
    "\n",
    "# 3. Dynamic Transaction Flagging\n",
    "# Merge transactions with register data to verify if specific txns are whitelisted\n",
    "# We perform a left join on (from, to) and then check the date window\n",
    "print(\"   -> Merging register data with transactions...\")\n",
    "df_trans_reg = df_trans[['from_acct', 'to_acct', 'day_idx']].merge(\n",
    "    df_reg[['from_acct', 'to_acct', 'start_date', 'end_date']],\n",
    "    on=['from_acct', 'to_acct'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# A transaction is registered if the link exists AND the date is within the window\n",
    "df_trans_reg['is_registered_txn'] = (\n",
    "    (df_trans_reg['start_date'].notna()) & \n",
    "    (df_trans_reg['day_idx'] >= df_trans_reg['start_date']) & \n",
    "    (df_trans_reg['day_idx'] <= df_trans_reg['end_date'])\n",
    ").astype(int)\n",
    "\n",
    "# Assign flag back to main dataframe and clean up\n",
    "df_trans['is_registered_txn'] = df_trans_reg['is_registered_txn']\n",
    "del df_trans_reg, df_reg # Free memory\n",
    "\n",
    "# --- Vectorized Out-degree Features ---\n",
    "print(\"Calculating out-degree features (Vectorized)...\")\n",
    "\n",
    "out_agg_funcs = {\n",
    "    'txn_amt': ['sum', 'mean', 'std', 'max', 'count'],\n",
    "    'to_acct': ['nunique'], \n",
    "    'day_idx': ['min', 'max', 'nunique'], \n",
    "    'is_late_night': ['sum', 'mean'], \n",
    "    'is_round_number_txn': ['mean'],\n",
    "    'is_registered_txn': ['sum', 'mean'] # [NEW] Added registered stats\n",
    "}\n",
    "\n",
    "# Dynamically add channel columns to aggregation\n",
    "for col in df_trans.columns:\n",
    "    if 'channel_' in col:\n",
    "        out_agg_funcs[col] = ['sum']\n",
    "\n",
    "out_features = df_trans.groupby('from_acct').agg(out_agg_funcs)\n",
    "out_features.columns = ['_'.join(col).strip() for col in out_features.columns.values]\n",
    "\n",
    "# Calculate account lifespan and rename columns for clarity\n",
    "out_features['account_lifespan_days'] = out_features['day_idx_max'] - out_features['day_idx_min']\n",
    "out_features = out_features.rename(columns={\n",
    "    'txn_amt_sum': 'total_out_amount', 'txn_amt_count': 'total_out_txns', \n",
    "    'txn_amt_mean': 'avg_out_amount', 'txn_amt_std': 'std_out_amount', \n",
    "    'txn_amt_max': 'max_out_amount', 'to_acct_nunique': 'unique_to_accts', \n",
    "    'day_idx_nunique': 'unique_out_txn_days', 'is_late_night_sum': 'late_night_out_txn_count', \n",
    "    'is_late_night_mean': 'late_night_out_txn_ratio', \n",
    "    'is_round_number_txn_mean': 'round_number_out_txn_ratio',\n",
    "    'is_registered_txn_sum': 'registered_out_txn_count', # [NEW]\n",
    "    'is_registered_txn_mean': 'registered_out_txn_ratio' # [NEW]\n",
    "}).drop(columns=['day_idx_min', 'day_idx_max'])\n",
    "\n",
    "# --- Vectorized In-degree Features ---\n",
    "print(\"Calculating in-degree features (Vectorized)...\")\n",
    "in_features = df_trans.groupby('to_acct').agg({\n",
    "    'txn_amt': ['sum', 'mean', 'count'], \n",
    "    'from_acct': ['nunique']\n",
    "})\n",
    "in_features.columns = ['in_' + '_'.join(col).strip() for col in in_features.columns.values]\n",
    "in_features = in_features.rename(columns={\n",
    "    'in_txn_amt_sum': 'total_in_amount', 'in_txn_amt_count': 'total_in_txns', \n",
    "    'in_txn_amt_mean': 'avg_in_amount', 'in_from_acct_nunique': 'unique_from_accts'\n",
    "})\n",
    "\n",
    "# --- Advanced Temporal Features (Linear Regression Slope) ---\n",
    "print(\"Extracting advanced temporal features (Vectorized Math)...\")\n",
    "\n",
    "# Calculate Slope using closed-form linear regression formula:\n",
    "# Slope = (N * Sum(XY) - Sum(X)*Sum(Y)) / (N * Sum(X^2) - Sum(X)^2)\n",
    "df_trans['xy'] = df_trans['day_idx'] * df_trans['txn_amt']\n",
    "df_trans['xx'] = df_trans['day_idx'] ** 2\n",
    "\n",
    "trend_stats = df_trans.groupby('from_acct').agg({\n",
    "    'txn_amt': ['sum', 'count'],\n",
    "    'day_idx': ['sum'],\n",
    "    'xy': ['sum'],\n",
    "    'xx': ['sum']\n",
    "})\n",
    "trend_stats.columns = ['y_sum', 'n', 'x_sum', 'xy_sum', 'xx_sum']\n",
    "\n",
    "epsilon = 1e-9\n",
    "numerator = (trend_stats['n'] * trend_stats['xy_sum']) - (trend_stats['x_sum'] * trend_stats['y_sum'])\n",
    "denominator = (trend_stats['n'] * trend_stats['xx_sum']) - (trend_stats['x_sum'] ** 2)\n",
    "trend_stats['amt_trend'] = numerator / (denominator + epsilon)\n",
    "\n",
    "# Activity Density\n",
    "span = out_features['account_lifespan_days'] + 1\n",
    "trend_stats['activity_density'] = out_features['unique_out_txn_days'] / span\n",
    "\n",
    "# Volatility (Coefficient of Variation)\n",
    "trend_stats['amt_volatility'] = out_features['std_out_amount'] / (out_features['avg_out_amount'] + epsilon)\n",
    "\n",
    "adv_temporal_features = trend_stats[['amt_trend', 'activity_density', 'amt_volatility']].fillna(0)\n",
    "del trend_stats, df_trans['xy'], df_trans['xx'] # Memory cleanup\n",
    "\n",
    "# --- Behavioral Anomaly Z-Scores ---\n",
    "print(\"Calculating behavioral anomaly Z-scores (Vectorized)...\")\n",
    "# Merge mean/std back to transactions to calculate deviation for every single transaction\n",
    "temp_stats = out_features[['avg_out_amount', 'std_out_amount']]\n",
    "df_trans = df_trans.merge(temp_stats, left_on='from_acct', right_index=True, how='left')\n",
    "df_trans['txn_zscore'] = (df_trans['txn_amt'] - df_trans['avg_out_amount']) / (df_trans['std_out_amount'].fillna(0) + epsilon)\n",
    "max_zscore_features = df_trans.groupby('from_acct')['txn_zscore'].max().to_frame(name='max_txn_zscore')\n",
    "\n",
    "# --- Sparse PageRank Calculation ---\n",
    "print(\"Calculating PageRank (Sparse Matrix Method)...\")\n",
    "# Map all unique accounts to integers 0..N\n",
    "all_accts = pd.unique(np.concatenate([df_trans['from_acct'].astype(str), df_trans['to_acct'].astype(str)]))\n",
    "acct_map = {acct: i for i, acct in enumerate(all_accts)}\n",
    "n_nodes = len(all_accts)\n",
    "\n",
    "# Build Sparse Matrix (COO format)\n",
    "from_idx = df_trans['from_acct'].map(acct_map).values\n",
    "to_idx = df_trans['to_acct'].map(acct_map).values\n",
    "data = np.ones(len(from_idx))\n",
    "A = sparse.coo_matrix((data, (from_idx, to_idx)), shape=(n_nodes, n_nodes), dtype=np.float32)\n",
    "\n",
    "pr_scores = fast_pagerank(A)\n",
    "pagerank_features = pd.DataFrame(pr_scores, index=all_accts, columns=['pagerank'])\n",
    "\n",
    "# --- Feature Integration ---\n",
    "print(\"Integrating all features...\")\n",
    "all_accts_df = pd.DataFrame(index=all_accts)\n",
    "node_features = all_accts_df.join(out_features)\\\n",
    "                            .join(in_features)\\\n",
    "                            .join(adv_temporal_features)\\\n",
    "                            .join(max_zscore_features)\\\n",
    "                            .join(pagerank_features)\\\n",
    "                            .join(reg_static_out)\\\n",
    "                            .join(reg_static_in) # [NEW] Join static register features\n",
    "\n",
    "node_features.fillna(0, inplace=True)\n",
    "\n",
    "# --- Interaction & Ratio Features ---\n",
    "print(\"Adding interaction features...\")\n",
    "node_features['net_flow'] = node_features['total_in_amount'] - node_features['total_out_amount']\n",
    "node_features['in_out_amount_ratio'] = node_features['total_in_amount'] / (node_features['total_out_amount'] + epsilon)\n",
    "node_features['hub_score'] = node_features['unique_from_accts'] * node_features['unique_to_accts']\n",
    "node_features['risk_weighted_volume'] = node_features['late_night_out_txn_ratio'] * node_features['total_out_amount']\n",
    "node_features['volatility_weighted_volume'] = node_features['amt_volatility'] * node_features['total_out_amount']\n",
    "node_features['pagerank_weighted_flow'] = node_features['pagerank'] * node_features['net_flow']\n",
    "node_features['avg_daily_out_txns'] = node_features['total_out_txns'] / (node_features['account_lifespan_days'] + epsilon)\n",
    "node_features['avg_daily_out_amount'] = node_features['total_out_amount'] / (node_features['account_lifespan_days'] + epsilon)\n",
    "node_features['avg_daily_in_txns'] = node_features['total_in_txns'] / (node_features['account_lifespan_days'] + epsilon)\n",
    "node_features['anomaly_magnitude'] = node_features['max_txn_zscore'] * node_features['avg_out_amount']\n",
    "\n",
    "# [NEW] Registered interaction features\n",
    "# Calculate \"Unregistered\" (Riskier) volume and counts\n",
    "node_features['unregistered_out_count'] = node_features['total_out_txns'] - node_features['registered_out_txn_count']\n",
    "node_features['unregistered_ratio'] = 1.0 - node_features['registered_out_txn_ratio']\n",
    "# High volatility combined with high unregistered ratio is suspicious\n",
    "node_features['risk_unregistered_volatility'] = node_features['unregistered_ratio'] * node_features['amt_volatility']\n",
    "\n",
    "# Final Cleanup\n",
    "node_features.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "node_features.fillna(0, inplace=True)\n",
    "print(f\"âœ… Final feature count: {node_features.shape[1]}\") \n",
    "\n",
    "# Garbage collection to free RAM before scaling\n",
    "del df_trans, out_features, in_features, adv_temporal_features, max_zscore_features, pagerank_features, A\n",
    "gc.collect()\n",
    "\n",
    "# --- 3. Feature Scaling ---\n",
    "print(\"\\nStep 3: Scaling node features...\")\n",
    "scaler = StandardScaler()\n",
    "# Convert to float32 to reduce memory footprint\n",
    "features_val = node_features.values.astype(np.float32)\n",
    "scaled_features_np = scaler.fit_transform(features_val)\n",
    "node_features_scaled = pd.DataFrame(scaled_features_np, index=node_features.index, columns=node_features.columns)\n",
    "\n",
    "# --- 4. Saving Artifacts ---\n",
    "print(\"\\nStep 4: Saving processed features to file...\")\n",
    "save_path = 'processed_features_with_interactions.pt' \n",
    "acct_to_idx = {acct_id: i for i, acct_id in enumerate(node_features_scaled.index)}\n",
    "feature_tensor = torch.tensor(node_features_scaled.values, dtype=torch.float)\n",
    "\n",
    "torch.save({\n",
    "    'feature_tensor': feature_tensor, \n",
    "    'acct_to_idx': acct_to_idx, \n",
    "    'feature_names': list(node_features.columns),\n",
    "    'scaler': scaler\n",
    "}, save_path)\n",
    "print(f\"âœ… Features saved to '{save_path}'\")\n",
    "print(\"\\n--- Completed Successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f436d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains and evaluates a GraphSAGE model for fraud detection.\n",
    "\n",
    "This script constitutes Part 2 of the pipeline, focusing on model training.\n",
    "It performs the following steps:\n",
    "1.Â  Loads node features (from Part 1) and raw data for graph/labels.\n",
    "2.Â  Prepares a PyTorch Geometric (PyG) `Data` object, including graph\n",
    "Â  Â  structure, features, labels, and train/val/test splits.\n",
    "3.Â  Defines a 3-layer GraphSAGE model with batch normalization and dropout.\n",
    "4.Â  Implements training and testing functions, using a dampened weighted\n",
    "Â  Â  CrossEntropyLoss to handle class imbalance.\n",
    "5.Â  Executes the training loop, tracking validation F1-score for\n",
    "Â  Â  early stopping and model checkpointing.\n",
    "6.Â  Loads the best-performing model and evaluates it on the hold-out\n",
    "Â  Â  test set.\n",
    "7.Â  Generates and displays plots of training metrics (loss, AUC, F1, etc.).\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### --- 0. Setup and Data Loading ---\n",
    "print(\"--- Part 2: Model Training (Optimized) ---\")\n",
    "print(\"Step 0: Setup and Data Loading...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "try:\n",
    "    # Load pre-processed features from Part 1\n",
    "    saved_data = torch.load('processed_features_with_interactions.pt', map_location=device, weights_only=False)\n",
    "    feature_tensor = saved_data['feature_tensor']\n",
    "    acct_to_idx = saved_data['acct_to_idx']\n",
    "    print(\"Pre-processed features loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'processed_features_with_interactions.pt' not found. Please run Part 1 first.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Load raw data for graph construction and labels\n",
    "    df_trans = pd.read_csv(\"data/acct_transaction.csv\", dtype={'from_acct': str, 'to_acct': str})\n",
    "    df_alert = pd.read_csv(\"data/acct_alert.csv\", dtype={'acct': str})\n",
    "    print(\"Raw transaction and alert data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading raw data files: {e}\")\n",
    "    exit()\n",
    "\n",
    "### --- 1. Prepare Graph Data and Splits ---\n",
    "print(\"\\nStep 1: Preparing Graph Data and Splits...\")\n",
    "# Filter transactions where both accounts are in our feature map\n",
    "valid_trans = df_trans[df_trans['from_acct'].isin(acct_to_idx) & df_trans['to_acct'].isin(acct_to_idx)]\n",
    "source_nodes = valid_trans['from_acct'].map(acct_to_idx).values\n",
    "target_nodes = valid_trans['to_acct'].map(acct_to_idx).values\n",
    "\n",
    "# Create edge index and make the graph undirected\n",
    "edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index)\n",
    "num_nodes = len(acct_to_idx)\n",
    "\n",
    "# Create labels (y) tensor\n",
    "y = torch.zeros(num_nodes, dtype=torch.long)\n",
    "alert_indices_list = [acct_to_idx[acct] for acct in df_alert['acct'] if acct in acct_to_idx]\n",
    "y[alert_indices_list] = 1\n",
    "\n",
    "# Create stratified train/validation/test splits\n",
    "indices = torch.arange(num_nodes)\n",
    "train_idx, temp_idx, y_train, y_temp = train_test_split(indices, y, train_size=0.7, stratify=y, random_state=42)\n",
    "val_idx, test_idx, _, _ = train_test_split(temp_idx, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Create boolean masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n",
    "\n",
    "# Create the PyG Data object\n",
    "graph_data = Data(x=feature_tensor, edge_index=edge_index, y=y,\n",
    "                  train_mask=train_mask, val_mask=val_mask, test_mask=test_mask).to(device)\n",
    "print(\"Graph data object created.\")\n",
    "\n",
    "### --- 2. GNN Model Definition ---\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3-layer GraphSAGE model with Batch Normalization and Dropout.\n",
    "\n",
    "    This model implements a stack of SAGEConv layers for node classification.\n",
    "    The architecture is:\n",
    "    SAGEConv -> BatchNorm -> ReLU -> Dropout\n",
    "    SAGEConv -> BatchNorm -> ReLU -> Dropout\n",
    "    SAGEConv (linear output)\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Dimensionality of the input node features.\n",
    "        hidden_channels (int): Dimensionality of the hidden embeddings.\n",
    "        out_channels (int): Dimensionality of the output (number of classes).\n",
    "        dropout (float): Dropout probability.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (SAGEConv): The first graph convolutional layer.\n",
    "        bn1 (nn.BatchNorm1d): Batch normalization for the first hidden layer.\n",
    "        conv2 (SAGEConv): The second graph convolutional layer.\n",
    "        bn2 (nn.BatchNorm1d): Batch normalization for the second hidden layer.\n",
    "        conv3 (SAGEConv): The output graph convolutional layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        \"\"\"Initializes the GraphSAGE model layers.\"\"\"\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the GraphSAGE model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input node feature tensor.\n",
    "            edge_index (torch.Tensor): The graph's edge index.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The raw output logits for each node.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "### --- 3. Training and Evaluation Flow ---\n",
    "print(\"\\nStep 3: Training and Evaluation...\")\n",
    "def train(model, data, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    Performs a single training step on the graph.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The GNN model to train.\n",
    "        data (Data): The PyG graph data object.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        loss_fn (torch.nn.Module): The loss function.\n",
    "\n",
    "    Returns:\n",
    "        float: The training loss for this step.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, mask):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a specified node mask (e.g., validation or test).\n",
    "\n",
    "    This function operates in `torch.no_grad()` mode.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The GNN model to evaluate.\n",
    "        data (Data): The PyG graph data object.\n",
    "        mask (torch.Tensor): The boolean mask (e.g., `data.val_mask` or\n",
    "                             `data.test_mask`) specifying which nodes\n",
    "                             to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (auc, recall, precision, f1) scores.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # Get probabilities for the positive class (class 1)\n",
    "    pred_proba = F.softmax(out[mask], dim=1)[:, 1]\n",
    "    # Get predicted classes (0 or 1) based on argmax\n",
    "    pred_class = out[mask].argmax(dim=1)\n",
    "    y_true = data.y[mask]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_true.cpu(), pred_proba.cpu())\n",
    "    recall = recall_score(y_true.cpu(), pred_class.cpu(), zero_division=0)\n",
    "    precision = precision_score(y_true.cpu(), pred_class.cpu(), zero_division=0)\n",
    "    f1 = f1_score(y_true.cpu(), pred_class.cpu(), zero_division=0)\n",
    "    return auc, recall, precision, f1\n",
    "\n",
    "# Calculate dampened class weights for imbalance\n",
    "num_positives = graph_data.y[graph_data.train_mask].sum().item()\n",
    "num_negatives = graph_data.train_mask.sum().item() - num_positives\n",
    "pos_weight_ratio = num_negatives / num_positives\n",
    "# Use sqrt to dampen the effect of the large weight\n",
    "dampened_weight = np.sqrt(pos_weight_ratio)\n",
    "class_weights = torch.tensor([1.0, dampened_weight], dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "print(f\"Original weight ratio: {pos_weight_ratio:.2f}\")\n",
    "print(f\"Dampened positive class weight (sqrt): {dampened_weight:.2f}\")\n",
    "\n",
    "\n",
    "# --- Hyperparameters and Initialization ---\n",
    "HIDDEN_CHANNELS = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 50000\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "model = GraphSAGE(in_channels=graph_data.num_node_features, hidden_channels=HIDDEN_CHANNELS, out_channels=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "# UPDATED: Initialize history with separate keys for Train and Val\n",
    "history = {\n",
    "    'epoch': [], \n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_auc': [], 'val_auc': [],\n",
    "    'train_recall': [], 'val_recall': [],\n",
    "    'train_precision': [], 'val_precision': [],\n",
    "    'train_f1': [], 'val_f1': []\n",
    "}\n",
    "best_val_f1 = -1\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train(model, graph_data, optimizer, loss_fn)\n",
    "    \n",
    "    # Validation and logging every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        # Evaluate on Validation set\n",
    "        val_auc, val_recall, val_precision, val_f1 = test(model, graph_data, graph_data.val_mask)\n",
    "        \n",
    "        # UPDATED: Evaluate on Training set\n",
    "        train_auc, train_recall, train_precision, train_f1 = test(model, graph_data, graph_data.train_mask)\n",
    "        \n",
    "        # UPDATED: Calculate Validation Loss explicitly\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(graph_data.x, graph_data.edge_index)\n",
    "            val_loss = loss_fn(out[graph_data.val_mask], graph_data.y[graph_data.val_mask]).item()\n",
    "\n",
    "        print(f'Epoch: {epoch:04d}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}')\n",
    "        \n",
    "        # Store metrics\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        history['train_recall'].append(train_recall)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        \n",
    "        history['train_precision'].append(train_precision)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        \n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        # Adjust learning rate based on validation F1\n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        current_metric = val_f1\n",
    "        if current_metric > best_val_f1:\n",
    "            best_val_f1 = current_metric\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\n--- Early stopping triggered at epoch {epoch} ---\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Total time: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "print(f\"Best validation F1-score: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "### --- 4. Final Evaluation ---\n",
    "print(\"\\nStep 4: Evaluating on Test Set...\")\n",
    "# Load the best model saved during training\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Evaluate on the test set using the default argmax (0.5) threshold\n",
    "final_auc, final_recall, final_precision, final_f1 = test(model, graph_data, graph_data.test_mask)\n",
    "\n",
    "print(f\"\\n--- Test Set Performance (using default argmax/0.5 threshold) ---\")\n",
    "print(f\"AUC: {final_auc:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"F1-Score: {final_f1:.4f}\")\n",
    "\n",
    "\n",
    "### --- 5. Visualization ---\n",
    "print(\"\\nStep 5: Visualizing Training Process (Train vs Val)...\")\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Training and Validation Metrics Comparison')\n",
    "\n",
    "# Plot 1: Loss\n",
    "ax[0, 0].plot(history['epoch'], history['train_loss'], label='Training Loss')\n",
    "ax[0, 0].plot(history['epoch'], history['val_loss'], label='Validation Loss')\n",
    "ax[0, 0].set_title('Loss')\n",
    "ax[0, 0].legend()\n",
    "ax[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: AUC\n",
    "ax[0, 1].plot(history['epoch'], history['train_auc'], label='Training AUC', color='blue')\n",
    "ax[0, 1].plot(history['epoch'], history['val_auc'], label='Validation AUC', color='orange')\n",
    "ax[0, 1].set_title('AUC')\n",
    "ax[0, 1].legend()\n",
    "ax[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: Recall & Precision\n",
    "# Solid line for Training, Dashed for Validation\n",
    "ax[1, 0].plot(history['epoch'], history['train_recall'], label='Train Recall', color='green', linestyle='-')\n",
    "ax[1, 0].plot(history['epoch'], history['val_recall'], label='Val Recall', color='green', linestyle='--')\n",
    "ax[1, 0].plot(history['epoch'], history['train_precision'], label='Train Precision', color='red', linestyle='-')\n",
    "ax[1, 0].plot(history['epoch'], history['val_precision'], label='Val Precision', color='red', linestyle='--')\n",
    "ax[1, 0].set_title('Recall & Precision')\n",
    "ax[1, 0].legend()\n",
    "ax[1, 0].grid(True)\n",
    "\n",
    "# Plot 4: F1-Score\n",
    "ax[1, 1].plot(history['epoch'], history['train_f1'], label='Training F1', color='purple', linestyle='-')\n",
    "ax[1, 1].plot(history['epoch'], history['val_f1'], label='Validation F1', color='purple', linestyle='--')\n",
    "ax[1, 1].set_title('F1-Score')\n",
    "ax[1, 1].legend()\n",
    "ax[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"\\n--- Part 2 Completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates predictions using a trained GraphSAGE model on a target list.\n",
    "\n",
    "This script executes Part 3 of the pipeline: Prediction.\n",
    "It performs the following steps:\n",
    "1.  Loads the pre-processed features and account mappings (from Part 1).\n",
    "2.  Loads the raw transaction data to reconstruct the full graph structure.\n",
    "3.  Loads the list of accounts requiring predictions.\n",
    "4.  Re-defines the GraphSAGE model architecture (must match Part 2).\n",
    "5.  Loads the saved weights of the best trained model (from Part 2).\n",
    "6.  Performs a forward pass on the entire graph to get probabilities for all\n",
    "    nodes.\n",
    "7.  Extracts the \"suspicious\" probabilities for the target accounts.\n",
    "8.  Applies an optimal threshold to classify accounts as 0 (normal) or\n",
    "    1 (suspicious).\n",
    "9.  Saves the final results to 'submission.csv'.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### --- 0. Environment Setup and Model Definition ---\n",
    "print(\"--- Part 3: Prediction ---\")\n",
    "print(\"Step 0: Setup Environment and Model Definition...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model definition (must be identical to Part 2)\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3-layer GraphSAGE model with Batch Normalization and Dropout.\n",
    "\n",
    "    This model implements a stack of SAGEConv layers for node classification.\n",
    "    The architecture is:\n",
    "    SAGEConv -> BatchNorm -> ReLU -> Dropout\n",
    "    SAGEConv -> BatchNorm -> ReLU -> Dropout\n",
    "    SAGEConv (linear output)\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Dimensionality of the input node features.\n",
    "        hidden_channels (int): Dimensionality of the hidden embeddings.\n",
    "        out_channels (int): Dimensionality of the output (number of classes).\n",
    "        dropout (float): Dropout probability.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (SAGEConv): The first graph convolutional layer.\n",
    "        bn1 (nn.BatchNorm1d): Batch normalization for the first hidden layer.\n",
    "        conv2 (SAGEConv): The second graph convolutional layer.\n",
    "        bn2 (nn.BatchNorm1d): Batch normalization for the second hidden layer.\n",
    "        conv3 (SAGEConv): The output graph convolutional layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        \"\"\"Initializes the GraphSAGE model layers.\"\"\"\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the GraphSAGE model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input node feature tensor.\n",
    "            edge_index (torch.Tensor): The graph's edge index.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The raw output logits for each node.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "### --- 1. Loading Necessary Data ---\n",
    "print(\"\\nStep 1: Loading necessary data...\")\n",
    "try:\n",
    "    # Load the correct feature file generated by Part 1 (with interaction features)\n",
    "    feature_file = 'processed_features_with_interactions.pt'\n",
    "    saved_data = torch.load(feature_file, map_location=device, weights_only=False)\n",
    "    \n",
    "    feature_tensor = saved_data['feature_tensor']\n",
    "    acct_to_idx = saved_data['acct_to_idx']\n",
    "    print(f\"Pre-processed features '{feature_file}' and mappings loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{feature_file}' not found. Please run Part 1 first.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # ðŸŽ¯ Ensure these paths are correct\n",
    "    predict_df = pd.read_csv(\"data/acct_predict.csv\", dtype={'acct': str})\n",
    "    df_trans = pd.read_csv(\"data/acct_transaction.csv\", dtype={'from_acct': str, 'to_acct': str})\n",
    "    print(\"Prediction account list and raw transactions loaded.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data files: {e}\")\n",
    "    exit()\n",
    "\n",
    "### --- 2. Reconstructing the Full Graph ---\n",
    "print(\"\\nStep 2: Reconstructing the full graph...\")\n",
    "# Ensure only accounts present in the feature mapping are used to build edges\n",
    "valid_trans = df_trans[df_trans['from_acct'].isin(acct_to_idx) & df_trans['to_acct'].isin(acct_to_idx)]\n",
    "source_nodes = valid_trans['from_acct'].map(acct_to_idx).values\n",
    "target_nodes = valid_trans['to_acct'].map(acct_to_idx).values\n",
    "edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index) # Maintain consistency with training\n",
    "\n",
    "# Create the Data object using the feature_tensor loaded in Step 1\n",
    "full_graph_data = Data(x=feature_tensor, edge_index=edge_index).to(device)\n",
    "print(f\"Full graph data object reconstructed ({full_graph_data.num_nodes} nodes).\")\n",
    "\n",
    "### --- 3. Loading Trained Model and Making Predictions ---\n",
    "print(\"\\nStep 3: Loading trained model and making predictions...\")\n",
    "# Hyperparameters must match those used in Part 2\n",
    "HIDDEN_CHANNELS = 128 \n",
    "model = GraphSAGE(in_channels=full_graph_data.num_node_features, \n",
    "                  hidden_channels=HIDDEN_CHANNELS, \n",
    "                  out_channels=2).to(device)\n",
    "\n",
    "try:\n",
    "    # Load the trained model weights from Part 2\n",
    "    model_file = 'best_model.pt'\n",
    "    model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    print(f\"Trained model weights loaded from '{model_file}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{model_file}' not found. Please run Part 2 to train the model first.\")\n",
    "    exit()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_logits = model(full_graph_data.x, full_graph_data.edge_index)\n",
    "    all_probs = F.softmax(all_logits, dim=1)\n",
    "print(\"Predictions generated for all nodes in the graph.\")\n",
    "\n",
    "### --- 4. Extracting Results and Generating Submission File ---\n",
    "print(\"\\nStep 4: Extracting results and generating submission file...\")\n",
    "\n",
    "OPTIMAL_THRESHOLD = 0.5\n",
    "print(f\"Using threshold: {OPTIMAL_THRESHOLD}\")\n",
    "\n",
    "# Get the \"class 1\" (suspicious) probabilities for all nodes\n",
    "all_pos_probs = all_probs[:, 1].cpu().numpy()\n",
    "\n",
    "# Get the indices for the accounts in the prediction list\n",
    "predict_indices = [acct_to_idx.get(acct, -1) for acct in predict_df['acct']]\n",
    "\n",
    "target_pos_probs = []\n",
    "for idx in predict_indices:\n",
    "    if idx != -1:\n",
    "        # If the account is in the graph, get its suspicious probability\n",
    "        target_pos_probs.append(all_pos_probs[idx])\n",
    "    else:\n",
    "        # If the account is not in the graph (e.g., a new account in 'acct_predict'),\n",
    "        # default its suspicious probability to 0.0 (label 0)\n",
    "        target_pos_probs.append(0.0)\n",
    "\n",
    "# Use the OPTIMAL_THRESHOLD to determine the final labels\n",
    "target_pos_probs_np = np.array(target_pos_probs)\n",
    "pred_labels = (target_pos_probs_np > OPTIMAL_THRESHOLD).astype(int)\n",
    "\n",
    "# Generate the submission file\n",
    "submission_df = predict_df[['acct']].copy()\n",
    "submission_df['label'] = pred_labels\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv created successfully!\")\n",
    "print(\"Predicted label distribution (based on threshold):\")\n",
    "print(pd.Series(pred_labels).value_counts())\n",
    "print(\"\\n--- Part 3 Completed ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
